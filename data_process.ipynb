{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据表转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 等分插值采样函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tansRSSI(data):\n",
    "    t = np.linspace(0,60,len(data))\n",
    "    rssi = np.array(data)\n",
    "    re_rssi = signal.resample(rssi,120)\n",
    "    t1 = np.linspace(0,60,120,endpoint=True)\n",
    "    return re_rssi.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINR 计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rssi_to_linear(rssi_dbm):\n",
    "    return 10 ** (rssi_dbm / 10)\n",
    "\n",
    "def linear_to_rssi(power_mw):\n",
    "    return 10 * np.log10(power_mw)\n",
    "\n",
    "def calculate_sinr(useful_rssi_dbm_list, interference_rssi_dbm_list, scale_factors):\n",
    "    noise_rssi_dbm = -100\n",
    "    noise_rssi_mw = rssi_to_linear(noise_rssi_dbm)\n",
    "    total_useful_rssi_mw = sum(rssi_to_linear(rssi) for rssi in useful_rssi_dbm_list)\n",
    "    total_interference_mw = 0\n",
    "    \n",
    "    for interference_rssi_dbm, scale in zip(interference_rssi_dbm_list, scale_factors):\n",
    "        interference_rssi_mw = rssi_to_linear(interference_rssi_dbm)\n",
    "        weighted_interference_mw = scale * interference_rssi_mw + (1 - scale) * noise_rssi_mw\n",
    "        total_interference_mw += weighted_interference_mw\n",
    "    \n",
    "    sinr_mw = total_useful_rssi_mw / total_interference_mw\n",
    "    sinr_db = linear_to_rssi(sinr_mw)\n",
    "    \n",
    "    return sinr_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ap的ap节点特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'training_set_2ap_loc0_nav82'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dict1['type'] = 0\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict1['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        ap_dict1['protocol'] = row['protocol']\n",
    "        ap_dict1['bss_id'] = row['bss_id']\n",
    "        ap_dict1['pd'] = row['pd']\n",
    "        ap_dict1['ed'] = row['ed']\n",
    "        ap_dict1['nav'] = row['nav']\n",
    "        ap_dict1['eirp'] = row['eirp']\n",
    "        ap_dict1['sinr'] = 0\n",
    "        if (ap_dict1['protocol'] == 'udp') :\n",
    "            ap_dict1['protocol'] = 1\n",
    "        elif (ap_dict1['protocol'] == 'tcp') :\n",
    "            ap_dict1['protocol'] = 2\n",
    "        \n",
    "        ap_dicts_list.append(ap_dict1)\n",
    "\n",
    "        ap_dict = {}\n",
    "        row = data[i+1]\n",
    "        ap_dict['type'] = 0\n",
    "        ap_dict['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        ap_dict['protocol'] = row['protocol']\n",
    "        ap_dict['bss_id'] = row['bss_id']\n",
    "        ap_dict['pd'] = row['pd']\n",
    "        ap_dict['ed'] = row['ed']\n",
    "        ap_dict['nav'] = row['nav']\n",
    "        ap_dict['eirp'] = row['eirp']\n",
    "        ap_dict['sinr'] = 0\n",
    "        if (ap_dict['protocol'] == 'udp') :\n",
    "            ap_dict['protocol'] = 1\n",
    "        elif (ap_dict['protocol'] == 'tcp') :\n",
    "            ap_dict['protocol'] = 2\n",
    "        \n",
    "        ap_dicts_list.append(ap_dict)\n",
    "        print(ap_dicts_list)\n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_ap/test\"+str(i//2)\n",
    "            filename = folder_path + \"/nodes_ap_\" + str(j).zfill(3) + \".csv\"\n",
    "            \n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=ap_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(ap_dicts_list)\n",
    "        ap_dicts_list.clear()\n",
    "        ap_dict1.clear()\n",
    "        ap_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ap的ap节点预测值 - 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'training_set_2ap_loc0_nav82'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict1['nss'] = float(row['nss'])\n",
    "        ap_dict1['mcs'] = float(row['mcs'])\n",
    "        ap_dict1['per'] = float(row['per'])\n",
    "        ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        ap_dict1['throughput'] = float(row['throughput'])\n",
    "        ap_dicts_list.append(ap_dict1)\n",
    "\n",
    "        ap_dict = {}\n",
    "        row = data[i+1]\n",
    "        ap_dict['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict['nss'] = float(row['nss'])\n",
    "        ap_dict['mcs'] = float(row['mcs'])\n",
    "        ap_dict['per'] = float(row['per'])\n",
    "        ap_dict['num_ampdu'] = float(row['num_ampdu'])\n",
    "        ap_dict['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        ap_dict['other_air_time'] = float(row['other_air_time'])\n",
    "        ap_dict['seq_time'] = float(row['seq_time'])\n",
    "        ap_dict['throughput'] = float(row['throughput'])\n",
    "        ap_dicts_list.append(ap_dict)\n",
    "        \n",
    "        print(ap_dicts_list)\n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_ap_predict/test\"+str(i//2)\n",
    "            filename = folder_path + \"/nodes_ap_predict_\" + str(j).zfill(3) + \".csv\"\n",
    "            \n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=ap_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(ap_dicts_list)\n",
    "        ap_dicts_list.clear()\n",
    "        ap_dict1.clear()\n",
    "        ap_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ap的ap节点预测值 - 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_2ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict1['nss'] = float(row['nss'])\n",
    "        else : ap_dict1['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict1['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict1['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict1['per'] = float(row['per'])\n",
    "        else : ap_dict1['per'] = 0\n",
    "        ap_dicts_list.append(ap_dict1)\n",
    "\n",
    "        ap_dict = {}\n",
    "        row = data[i+1]\n",
    "        ap_dict['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict['nss'] = float(row['nss'])\n",
    "        else : ap_dict['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict['per'] = float(row['per'])\n",
    "        else : ap_dict['per'] = 0\n",
    "        ap_dicts_list.append(ap_dict)\n",
    "        \n",
    "        print(ap_dicts_list)\n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_ap_predict/test\"+str(i//2)\n",
    "            filename = folder_path + \"/nodes_ap_predict_\" + str(j).zfill(3) + \".csv\"\n",
    "            \n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=ap_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(ap_dicts_list)\n",
    "        ap_dicts_list.clear()\n",
    "        ap_dict1.clear()\n",
    "        ap_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ap的ap节点特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_1_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dict1['type'] = 0\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict1['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        ap_dict1['protocol'] = row['protocol']\n",
    "        ap_dict1['bss_id'] = row['bss_id']\n",
    "        ap_dict1['pd'] = row['pd']\n",
    "        ap_dict1['ed'] = row['ed']\n",
    "        ap_dict1['nav'] = row['nav']\n",
    "        ap_dict1['eirp'] = row['eirp']\n",
    "        ap_dict1['sinr'] = 0\n",
    "        if (ap_dict1['protocol'] == 'udp') :\n",
    "            ap_dict1['protocol'] = 1\n",
    "        elif (ap_dict1['protocol'] == 'tcp') :\n",
    "            ap_dict1['protocol'] = 2\n",
    "        \n",
    "        ap_dicts_list.append(ap_dict1)\n",
    "\n",
    "        ap_dict = {}\n",
    "        row = data[i+1]\n",
    "        ap_dict['type'] = 0\n",
    "        ap_dict['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        ap_dict['protocol'] = row['protocol']\n",
    "        ap_dict['bss_id'] = row['bss_id']\n",
    "        ap_dict['pd'] = row['pd']\n",
    "        ap_dict['ed'] = row['ed']\n",
    "        ap_dict['nav'] = row['nav']\n",
    "        ap_dict['eirp'] = row['eirp']\n",
    "        ap_dict['sinr'] = 0\n",
    "        if (ap_dict['protocol'] == 'udp') :\n",
    "            ap_dict['protocol'] = 1\n",
    "        elif (ap_dict['protocol'] == 'tcp') :\n",
    "            ap_dict['protocol'] = 2\n",
    "\n",
    "        ap_dicts_list.append(ap_dict)\n",
    "        \n",
    "        ap_dict2 = {}\n",
    "        row = data[i+2]\n",
    "        ap_dict2['type'] = 0\n",
    "        ap_dict2['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        ap_dict2['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        ap_dict2['protocol'] = row['protocol']\n",
    "        ap_dict2['bss_id'] = row['bss_id']\n",
    "        ap_dict2['pd'] = row['pd']\n",
    "        ap_dict2['ed'] = row['ed']\n",
    "        ap_dict2['nav'] = row['nav']\n",
    "        ap_dict2['eirp'] = row['eirp']\n",
    "        ap_dict2['sinr'] = 0\n",
    "        if (ap_dict2['protocol'] == 'udp') :\n",
    "            ap_dict2['protocol'] = 1\n",
    "        elif (ap_dict2['protocol'] == 'tcp') :\n",
    "            ap_dict2['protocol'] = 2\n",
    "        \n",
    "        ap_dicts_list.append(ap_dict2)\n",
    "        \n",
    "        print(ap_dicts_list)\n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_ap/test\"+str(i//3)\n",
    "            filename = folder_path + \"/nodes_ap_\" + str(j).zfill(3) + \".csv\"\n",
    "            \n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=ap_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(ap_dicts_list)\n",
    "        ap_dicts_list.clear()\n",
    "        ap_dict1.clear()\n",
    "        ap_dict2.clear()\n",
    "        ap_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ap的ap节点预测值 - 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict1['nss'] = float(row['nss'])\n",
    "        else : ap_dict1['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict1['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict1['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict1['per'] = float(row['per'])\n",
    "        else : ap_dict1['per'] = 0\n",
    "        ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        ap_dict1['throughput'] = float(row['throughput'])\n",
    "        ap_dicts_list.append(ap_dict1)\n",
    "\n",
    "        ap_dict = {}\n",
    "        row = data[i+1]\n",
    "        ap_dict['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict['nss'] = float(row['nss'])\n",
    "        else : ap_dict['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict['per'] = float(row['per'])\n",
    "        else : ap_dict['per'] = 0\n",
    "        ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        ap_dict1['throughput'] = float(row['throughput'])\n",
    "\n",
    "        ap_dicts_list.append(ap_dict)\n",
    "        \n",
    "        ap_dict2 = {}\n",
    "        row = data[i+2]\n",
    "        ap_dict2['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict2['nss'] = float(row['nss'])\n",
    "        else : ap_dict2['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict2['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict2['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict2['per'] = float(row['per'])\n",
    "        else : ap_dict2['per'] = 0\n",
    "        ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        ap_dict1['throughput'] = float(row['throughput'])\n",
    "        \n",
    "        ap_dicts_list.append(ap_dict2)\n",
    "        \n",
    "        print(ap_dicts_list)\n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_ap_predict/test\"+str(i//3)\n",
    "            filename = folder_path + \"/nodes_ap_predict_\" + str(j).zfill(3) + \".csv\"\n",
    "            \n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=ap_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(ap_dicts_list)\n",
    "        ap_dicts_list.clear()\n",
    "        ap_dict1.clear()\n",
    "        ap_dict2.clear()\n",
    "        ap_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ap的ap节点预测值 - 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dicts_list = []\n",
    "        ap_dict1 = {}\n",
    "        row = data[i]\n",
    "        ap_dict1['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict1['nss'] = float(row['nss'])\n",
    "        else : ap_dict1['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict1['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict1['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict1['per'] = float(row['per'])\n",
    "        else : ap_dict1['per'] = 0\n",
    "        # ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        # ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        # ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        # ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        # ap_dict1['throughput'] = float(row['throughput'])\n",
    "        ap_dicts_list.append(ap_dict1)\n",
    "\n",
    "        ap_dict = {}\n",
    "        row = data[i+1]\n",
    "        ap_dict['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict['nss'] = float(row['nss'])\n",
    "        else : ap_dict['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict['per'] = float(row['per'])\n",
    "        else : ap_dict['per'] = 0\n",
    "        # ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        # ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        # ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        # ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        # ap_dict1['throughput'] = float(row['throughput'])\n",
    "\n",
    "        ap_dicts_list.append(ap_dict)\n",
    "        \n",
    "        ap_dict2 = {}\n",
    "        row = data[i+2]\n",
    "        ap_dict2['id'] = int(row['ap_id'][3:len(row['ap_id'])])\n",
    "        if(row['nss']!=\"\") : ap_dict2['nss'] = float(row['nss'])\n",
    "        else : ap_dict2['nss'] = 0\n",
    "        if(row['mcs']!=\"\") : ap_dict2['mcs'] = float(row['mcs'])\n",
    "        else : ap_dict2['mcs'] = 0\n",
    "        if(row['per']!=\"\") : ap_dict2['per'] = float(row['per'])\n",
    "        else : ap_dict2['per'] = 0\n",
    "        # ap_dict1['num_ampdu'] = float(row['num_ampdu'])\n",
    "        # ap_dict1['ppdu_dur'] = float(row['ppdu_dur'])\n",
    "        # ap_dict1['other_air_time'] = float(row['other_air_time'])\n",
    "        # ap_dict1['seq_time'] = float(row['seq_time'])\n",
    "        # ap_dict1['throughput'] = float(row['throughput'])\n",
    "        \n",
    "        ap_dicts_list.append(ap_dict2)\n",
    "        \n",
    "        print(ap_dicts_list)\n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_ap_predict/test\"+str(i//3)\n",
    "            filename = folder_path + \"/nodes_ap_predict_\" + str(j).zfill(3) + \".csv\"\n",
    "            \n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=ap_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(ap_dicts_list)\n",
    "        ap_dicts_list.clear()\n",
    "        ap_dict1.clear()\n",
    "        ap_dict2.clear()\n",
    "        ap_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两节点的 ap-ap 边特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_2ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "# print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(data[i]['ap_from_ap_0_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_sum_ant_rssi']))\n",
    "    \n",
    "    if(data[i]['sta_from_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_sum_ant_rssi']))\n",
    "\n",
    "    if(data[i]['sta_to_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_sum_ant_rssi']))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        # print(len(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "        for j in range(120):         \n",
    "            folder_path = \"./out/\"+src_file_name+\"/edge_ap_ap/test\"+str(i//2)\n",
    "            filename = folder_path + \"/ap_ap_\" + str(j).zfill(3) + \".csv\"\n",
    "            edges_ap_ap = []\n",
    "            \n",
    "            dict01 = {}\n",
    "            dict01['src']=0\n",
    "            dict01['dst']=1\n",
    "            dict01['rssi_sum']=data[i]['ap_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict01['rssi_mean']=data[i]['ap_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict01['rssi_max']=data[i]['ap_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict01)\n",
    "            \n",
    "            dict10 = {}\n",
    "            dict10['src']=1\n",
    "            dict10['dst']=0\n",
    "            dict10['rssi_sum']=data[i+1]['ap_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict10['rssi_mean']=data[i+1]['ap_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict10['rssi_max']=data[i+1]['ap_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict10)\n",
    "\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=edges_ap_ap[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(edges_ap_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三节点的 ap-ap 边特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "# print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(data[i]['ap_from_ap_0_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_sum_ant_rssi'] = [-100.0]*120\n",
    "    \n",
    "    if(data[i]['sta_from_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_2_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_2_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_2_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_sum_ant_rssi']))\n",
    "\n",
    "    if(data[i]['sta_to_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_2_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_2_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_2_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_sum_ant_rssi']))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        print(len(data[i+1]['ap_from_ap_0_sum_ant_rssi']))\n",
    "        for j in range(120):         \n",
    "            folder_path = \"./out/\"+src_file_name+\"/edge_ap_ap/test\"+str(i//3)\n",
    "            filename = folder_path + \"/ap_ap_\" + str(j).zfill(3) + \".csv\"\n",
    "            edges_ap_ap = []\n",
    "            \n",
    "            dict01 = {}\n",
    "            dict01['src']=0\n",
    "            dict01['dst']=1\n",
    "            dict01['rssi_sum']=data[i+1]['ap_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict01['rssi_mean']=data[i+1]['ap_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict01['rssi_max']=data[i+1]['ap_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict01)\n",
    "\n",
    "            dict02 = {}\n",
    "            dict02['src']=0\n",
    "            dict02['dst']=2\n",
    "            dict02['rssi_sum']=data[i]['ap_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict02['rssi_mean']=data[i]['ap_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict02['rssi_max']=data[i]['ap_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict02)\n",
    "            \n",
    "            dict10 = {}\n",
    "            dict10['src']=1\n",
    "            dict10['dst']=0\n",
    "            dict10['rssi_sum']=data[i+2]['ap_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict10['rssi_mean']=data[i+2]['ap_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict10['rssi_max']=data[i+2]['ap_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict10)\n",
    "            \n",
    "            dict12 = {}\n",
    "            dict12['src']=1\n",
    "            dict12['dst']=2\n",
    "            dict12['rssi_sum']=data[i]['ap_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict12['rssi_mean']=data[i]['ap_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict12['rssi_max']=data[i]['ap_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict12)\n",
    "            \n",
    "            dict20 = {}\n",
    "            dict20['src']=2\n",
    "            dict20['dst']=0\n",
    "            dict20['rssi_sum']=data[i+2]['ap_from_ap_2_sum_ant_rssi'][j]\n",
    "            dict20['rssi_mean']=data[i+2]['ap_from_ap_2_mean_ant_rssi'][j]\n",
    "            dict20['rssi_max']=data[i+2]['ap_from_ap_2_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict20)\n",
    "            \n",
    "            dict21 = {}\n",
    "            dict21['src']=2\n",
    "            dict21['dst']=1\n",
    "            dict21['rssi_sum']=data[i+1]['ap_from_ap_2_sum_ant_rssi'][j]\n",
    "            dict21['rssi_mean']=data[i+1]['ap_from_ap_2_mean_ant_rssi'][j]\n",
    "            dict21['rssi_max']=data[i+1]['ap_from_ap_2_max_ant_rssi'][j]\n",
    "            edges_ap_ap.append(dict21)\n",
    "\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=edges_ap_ap[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(edges_ap_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两节点的 AP-STA 边属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_2ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "# print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(data[i]['ap_from_ap_0_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_sum_ant_rssi']))\n",
    "    \n",
    "    if(data[i]['sta_from_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_sum_ant_rssi']))\n",
    "\n",
    "    if(data[i]['sta_to_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_sum_ant_rssi']))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        # print(len(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "        for j in range(120):         \n",
    "            folder_path = \"./out/\"+src_file_name+\"/edge_ap_sta/test\"+str(i//2)\n",
    "            filename = folder_path + \"/ap_sta_\" + str(j).zfill(3) + \".csv\"\n",
    "            edges_ap_sta = []\n",
    "            \n",
    "            dict00 = {}\n",
    "            dict00['src']=0\n",
    "            dict00['dst']=0\n",
    "            dict00['rssi_sum']=data[i+1]['sta_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict00['rssi_mean']=data[i+1]['sta_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict00['rssi_max']=data[i+1]['sta_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict00)\n",
    "            \n",
    "            dict10 = {}\n",
    "            dict10['src']=1\n",
    "            dict10['dst']=0\n",
    "            dict10['rssi_sum']=data[i+1]['sta_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict10['rssi_mean']=data[i+1]['sta_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict10['rssi_max']=data[i+1]['sta_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict10)\n",
    "\n",
    "            dict01 = {}\n",
    "            dict01['src']=0\n",
    "            dict01['dst']=1\n",
    "            dict01['rssi_sum']=data[i]['sta_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict01['rssi_mean']=data[i]['sta_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict01['rssi_max']=data[i]['sta_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict01)\n",
    "            \n",
    "            dict11 = {}\n",
    "            dict11['src']=1\n",
    "            dict11['dst']=1\n",
    "            dict11['rssi_sum']=data[i]['sta_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict11['rssi_mean']=data[i]['sta_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict11['rssi_max']=data[i]['sta_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict11)\n",
    "\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=edges_ap_sta[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(edges_ap_sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三节点的AP-STA边特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "# print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(data[i]['ap_from_ap_0_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_sum_ant_rssi'] = [-100.0]*120\n",
    "    \n",
    "    if(data[i]['sta_from_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_2_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_2_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_2_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_sum_ant_rssi']))\n",
    "\n",
    "    if(data[i]['sta_to_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_2_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_2_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_2_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_sum_ant_rssi']))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        # print(len(data[i+1]['sta_from_ap_2_sum_ant_rssi']))\n",
    "        for j in range(120):         \n",
    "            folder_path = \"./out/\"+src_file_name+\"/edge_ap_sta/test\"+str(i//3)\n",
    "            filename = folder_path + \"/ap_sta_\" + str(j).zfill(3) + \".csv\"\n",
    "            edges_ap_sta = []\n",
    "            \n",
    "            dict00 = {}\n",
    "            dict00['src']=0\n",
    "            dict00['dst']=0\n",
    "            dict00['rssi_sum']=data[i+2]['sta_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict00['rssi_mean']=data[i+2]['sta_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict00['rssi_max']=data[i+2]['sta_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict00)\n",
    "            \n",
    "            dict10 = {}\n",
    "            dict10['src']=1\n",
    "            dict10['dst']=0\n",
    "            dict10['rssi_sum']=data[i+2]['sta_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict10['rssi_mean']=data[i+2]['sta_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict10['rssi_max']=data[i+2]['sta_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict10)\n",
    "\n",
    "            dict20 = {}\n",
    "            dict20['src']=2\n",
    "            dict20['dst']=0\n",
    "            dict20['rssi_sum']=data[i+2]['sta_from_ap_2_sum_ant_rssi'][j]\n",
    "            dict20['rssi_mean']=data[i+2]['sta_from_ap_2_mean_ant_rssi'][j]\n",
    "            dict20['rssi_max']=data[i+2]['sta_from_ap_2_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict20)\n",
    "\n",
    "            dict01 = {}\n",
    "            dict01['src']=0\n",
    "            dict01['dst']=1\n",
    "            dict01['rssi_sum']=data[i+1]['sta_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict01['rssi_mean']=data[i+1]['sta_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict01['rssi_max']=data[i+1]['sta_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict01)\n",
    "            \n",
    "            dict11 = {}\n",
    "            dict11['src']=1\n",
    "            dict11['dst']=1\n",
    "            dict11['rssi_sum']=data[i+1]['sta_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict11['rssi_mean']=data[i+1]['sta_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict11['rssi_max']=data[i+1]['sta_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict11)\n",
    "\n",
    "            dict21 = {}\n",
    "            dict21['src']=2\n",
    "            dict21['dst']=1\n",
    "            dict21['rssi_sum']=data[i+1]['sta_from_ap_2_sum_ant_rssi'][j]\n",
    "            dict21['rssi_mean']=data[i+1]['sta_from_ap_2_mean_ant_rssi'][j]\n",
    "            dict21['rssi_max']=data[i+1]['sta_from_ap_2_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict21)\n",
    "            \n",
    "            dict02 = {}\n",
    "            dict02['src']=0\n",
    "            dict02['dst']=2\n",
    "            dict02['rssi_sum']=data[i]['sta_from_ap_0_sum_ant_rssi'][j]\n",
    "            dict02['rssi_mean']=data[i]['sta_from_ap_0_mean_ant_rssi'][j]\n",
    "            dict02['rssi_max']=data[i]['sta_from_ap_0_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict02)\n",
    "            \n",
    "            dict12 = {}\n",
    "            dict12['src']=1\n",
    "            dict12['dst']=2\n",
    "            dict12['rssi_sum']=data[i]['sta_from_ap_1_sum_ant_rssi'][j]\n",
    "            dict12['rssi_mean']=data[i]['sta_from_ap_1_mean_ant_rssi'][j]\n",
    "            dict12['rssi_max']=data[i]['sta_from_ap_1_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict12)\n",
    "            \n",
    "            dict22 = {}\n",
    "            dict22['src']=2\n",
    "            dict22['dst']=2\n",
    "            dict22['rssi_sum']=data[i]['sta_from_ap_2_sum_ant_rssi'][j]\n",
    "            dict22['rssi_mean']=data[i]['sta_from_ap_2_mean_ant_rssi'][j]\n",
    "            dict22['rssi_max']=data[i]['sta_from_ap_2_max_ant_rssi'][j]\n",
    "            edges_ap_sta.append(dict22)\n",
    "\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=edges_ap_sta[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(edges_ap_sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两节点的 STA-AP 边特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_2ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "# print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(data[i]['ap_from_ap_0_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['ap_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_sum_ant_rssi']))\n",
    "    \n",
    "    if(data[i]['sta_from_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_sum_ant_rssi']))\n",
    "\n",
    "    if(data[i]['sta_to_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_max_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_mean_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_sum_ant_rssi']))\n",
    "    if(data[i]['sta_to_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_sum_ant_rssi']))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        # print(len(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "        for j in range(120):         \n",
    "            folder_path = \"./out/\"+src_file_name+\"/edge_sta_ap/test\"+str(i//2)\n",
    "            filename = folder_path + \"/sta_ap_\" + str(j).zfill(3) + \".csv\"\n",
    "            edges_sta_ap = []\n",
    "            \n",
    "            dict00 = {}\n",
    "            dict00['src']=0\n",
    "            dict00['dst']=0\n",
    "            dict00['rssi_sum']=data[i+1]['sta_to_ap_0_sum_ant_rssi'][j]\n",
    "            dict00['rssi_mean']=data[i+1]['sta_to_ap_0_mean_ant_rssi'][j]\n",
    "            dict00['rssi_max']=data[i+1]['sta_to_ap_0_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict00)\n",
    "\n",
    "            dict01 = {}\n",
    "            dict01['src']=0\n",
    "            dict01['dst']=1\n",
    "            dict01['rssi_sum']=data[i+1]['sta_to_ap_1_sum_ant_rssi'][j]\n",
    "            dict01['rssi_mean']=data[i+1]['sta_to_ap_1_mean_ant_rssi'][j]\n",
    "            dict01['rssi_max']=data[i+1]['sta_to_ap_1_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict01)\n",
    "            \n",
    "            dict10 = {}\n",
    "            dict10['src']=1\n",
    "            dict10['dst']=0\n",
    "            dict10['rssi_sum']=data[i]['sta_to_ap_0_sum_ant_rssi'][j]\n",
    "            dict10['rssi_mean']=data[i]['sta_to_ap_0_mean_ant_rssi'][j]\n",
    "            dict10['rssi_max']=data[i]['sta_to_ap_0_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict10)\n",
    "\n",
    "            \n",
    "            dict11 = {}\n",
    "            dict11['src']=1\n",
    "            dict11['dst']=1\n",
    "            dict11['rssi_sum']=data[i]['sta_to_ap_1_sum_ant_rssi'][j]\n",
    "            dict11['rssi_mean']=data[i]['sta_to_ap_1_mean_ant_rssi'][j]\n",
    "            dict11['rssi_max']=data[i]['sta_to_ap_1_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict11)\n",
    "\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=edges_sta_ap[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(edges_sta_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三节点的 STA-AP 边特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "# print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(data[i]['ap_from_ap_0_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_max_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_max_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_mean_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_mean_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_0_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_0_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_1_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_1_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['ap_from_ap_2_sum_ant_rssi'] != \"\") : data[i]['ap_from_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['ap_from_ap_2_sum_ant_rssi']))\n",
    "    else : data[i]['ap_from_ap_2_sum_ant_rssi'] = [-100.0]*120\n",
    "    \n",
    "    if(data[i]['sta_from_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_max_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_0_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_max_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_1_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_2_max_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_max_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_2_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_mean_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_0_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_mean_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_1_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_2_mean_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_mean_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_2_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_0_sum_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_0_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_1_sum_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_1_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_from_ap_2_sum_ant_rssi'] != \"\") : data[i]['sta_from_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_from_ap_2_sum_ant_rssi']))\n",
    "    else : data[i]['sta_from_ap_2_sum_ant_rssi'] = [-100.0]*120\n",
    "\n",
    "    if(data[i]['sta_to_ap_0_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_max_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_0_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_1_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_max_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_1_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_2_max_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_max_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_max_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_2_max_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_0_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_mean_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_0_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_1_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_mean_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_1_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_2_mean_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_mean_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_mean_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_2_mean_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_0_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_0_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_0_sum_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_0_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_1_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_1_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_1_sum_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_1_sum_ant_rssi'] = [-100.0]*120\n",
    "    if(data[i]['sta_to_ap_2_sum_ant_rssi'] != \"\") : data[i]['sta_to_ap_2_sum_ant_rssi'] = tansRSSI(ast.literal_eval(data[i]['sta_to_ap_2_sum_ant_rssi']))\n",
    "    else : data[i]['sta_to_ap_2_sum_ant_rssi'] = [-100.0]*120\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        # print(len(data[i+1]['sta_from_ap_2_sum_ant_rssi']))\n",
    "        for j in range(120):         \n",
    "            folder_path = \"./out/\"+src_file_name+\"/edge_sta_ap/test\"+str(i//3)\n",
    "            filename = folder_path + \"/sta_ap_\" + str(j).zfill(3) + \".csv\"\n",
    "            edges_sta_ap = []\n",
    "            \n",
    "            dict00 = {}\n",
    "            dict00['src']=0\n",
    "            dict00['dst']=0\n",
    "            dict00['rssi_sum']=data[i+2]['sta_to_ap_0_sum_ant_rssi'][j]\n",
    "            dict00['rssi_mean']=data[i+2]['sta_to_ap_0_mean_ant_rssi'][j]\n",
    "            dict00['rssi_max']=data[i+2]['sta_to_ap_0_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict00)\n",
    "            \n",
    "            dict01 = {}\n",
    "            dict01['src']=0\n",
    "            dict01['dst']=1\n",
    "            dict01['rssi_sum']=data[i+2]['sta_to_ap_1_sum_ant_rssi'][j]\n",
    "            dict01['rssi_mean']=data[i+2]['sta_to_ap_1_mean_ant_rssi'][j]\n",
    "            dict01['rssi_max']=data[i+2]['sta_to_ap_1_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict01)\n",
    "            \n",
    "            dict02 = {}\n",
    "            dict02['src']=0\n",
    "            dict02['dst']=2\n",
    "            print(len(data[i+2]['sta_to_ap_2_sum_ant_rssi']))\n",
    "            dict02['rssi_sum']=data[i+2]['sta_to_ap_2_sum_ant_rssi'][j]\n",
    "            dict02['rssi_mean']=data[i+2]['sta_to_ap_2_mean_ant_rssi'][j]\n",
    "            dict02['rssi_max']=data[i+2]['sta_to_ap_2_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict02)\n",
    "\n",
    "            dict10 = {}\n",
    "            dict10['src']=1\n",
    "            dict10['dst']=0\n",
    "            dict10['rssi_sum']=data[i+1]['sta_to_ap_0_sum_ant_rssi'][j]\n",
    "            dict10['rssi_mean']=data[i+1]['sta_to_ap_0_mean_ant_rssi'][j]\n",
    "            dict10['rssi_max']=data[i+1]['sta_to_ap_0_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict10)\n",
    "            \n",
    "            dict11 = {}\n",
    "            dict11['src']=1\n",
    "            dict11['dst']=1\n",
    "            dict11['rssi_sum']=data[i+1]['sta_to_ap_1_sum_ant_rssi'][j]\n",
    "            dict11['rssi_mean']=data[i+1]['sta_to_ap_1_mean_ant_rssi'][j]\n",
    "            dict11['rssi_max']=data[i+1]['sta_to_ap_1_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict11)           \n",
    "\n",
    "            dict12 = {}\n",
    "            dict12['src']=1\n",
    "            dict12['dst']=2\n",
    "            dict12['rssi_sum']=data[i+1]['sta_to_ap_2_sum_ant_rssi'][j]\n",
    "            dict12['rssi_mean']=data[i+1]['sta_to_ap_2_mean_ant_rssi'][j]\n",
    "            dict12['rssi_max']=data[i+1]['sta_to_ap_2_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict12)\n",
    "            \n",
    "            dict20 = {}\n",
    "            dict20['src']=2\n",
    "            dict20['dst']=0\n",
    "            dict20['rssi_sum']=data[i]['sta_to_ap_0_sum_ant_rssi'][j]\n",
    "            dict20['rssi_mean']=data[i]['sta_to_ap_0_mean_ant_rssi'][j]\n",
    "            dict20['rssi_max']=data[i]['sta_to_ap_0_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict20)\n",
    "            \n",
    "            dict21 = {}\n",
    "            dict21['src']=2\n",
    "            dict21['dst']=1\n",
    "            dict21['rssi_sum']=data[i]['sta_to_ap_1_sum_ant_rssi'][j]\n",
    "            dict21['rssi_mean']=data[i]['sta_to_ap_1_mean_ant_rssi'][j]\n",
    "            dict21['rssi_max']=data[i]['sta_to_ap_1_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict21)                  \n",
    "            \n",
    "            dict22 = {}\n",
    "            dict22['src']=2\n",
    "            dict22['dst']=2\n",
    "            dict22['rssi_sum']=data[i]['sta_to_ap_2_sum_ant_rssi'][j]\n",
    "            dict22['rssi_mean']=data[i]['sta_to_ap_2_mean_ant_rssi'][j]\n",
    "            dict22['rssi_max']=data[i]['sta_to_ap_2_max_ant_rssi'][j]\n",
    "            edges_sta_ap.append(dict22)\n",
    "\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=edges_sta_ap[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(edges_sta_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两节点的 sta 节点特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edge_by_src_dst(edge_ap_sta, src_value, dst_value):\n",
    "    for edge in edge_ap_sta:\n",
    "        # print(type(edge['src']),type(edge['dst']),type(src_value),type(dst_value))\n",
    "        if int(edge['src']) == src_value and int(edge['dst']) == dst_value:\n",
    "            return edge\n",
    "    return None  # 如果未找到则返回None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_2ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%2==0) :\n",
    "        sta_dicts_list = []\n",
    "        sta_dict1 = {}\n",
    "        \n",
    "        row = data[i]\n",
    "        sta_dict1['type'] = 1\n",
    "        sta_dict1['id'] = int(row['sta_id'][4:len(row['sta_id'])])\n",
    "        sta_dict1['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        sta_dict1['protocol'] = 0\n",
    "        sta_dict1['bss_id'] = row['bss_id']\n",
    "        sta_dict1['pd'] = int(row['pd'])\n",
    "        sta_dict1['ed'] = int(row['ed'])\n",
    "        sta_dict1['nav'] = int(row['nav'])\n",
    "        sta_dict1['eirp'] = 0\n",
    "        sta_dict1['sinr'] = 0\n",
    "        \n",
    "        sta_dicts_list.append(sta_dict1)\n",
    "\n",
    "        sta_dict = {}\n",
    "        row = data[i+1]\n",
    "        sta_dict['type'] = 1\n",
    "        sta_dict['id'] = int(row['sta_id'][4:len(row['sta_id'])])\n",
    "        sta_dict['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        sta_dict['protocol'] = 0\n",
    "        sta_dict['bss_id'] = row['bss_id']\n",
    "        sta_dict['pd'] = int(row['pd'])\n",
    "        sta_dict['ed'] = int(row['ed'])\n",
    "        sta_dict['nav'] = int(row['nav']) \n",
    "        sta_dict['eirp'] = 0\n",
    "        sta_dict['sinr'] = 0\n",
    "             \n",
    "        sta_dicts_list.append(sta_dict)\n",
    "        \n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_sta/test\"+str(i//2)\n",
    "            filename = folder_path + \"/nodes_sta_\" + str(j).zfill(3) + \".csv\"\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            edge_ap_sta = []\n",
    "            with codecs.open('./out/'+src_file_name+'/edge_ap_sta/test'+str(i//2)+'/ap_sta_'+str(j).zfill(3)+'.csv',encoding='utf-8-sig') as f:\n",
    "                for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                    edge_ap_sta.append(row)\n",
    "            edge_ap_ap = []\n",
    "            with codecs.open('./out/'+src_file_name+'/edge_ap_ap/test'+str(i//2)+'/ap_ap_'+str(j).zfill(3)+'.csv',encoding='utf-8-sig') as f:\n",
    "                for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                    edge_ap_ap.append(row)\n",
    "            \n",
    "            edge_ap_sta_01 = find_edge_by_src_dst(edge_ap_sta,0,1)            \n",
    "            sta_from_ap_0_sum_rssi = float(edge_ap_sta_01['rssi_sum'])\n",
    "            edge_ap_sta_11 = find_edge_by_src_dst(edge_ap_sta,1,1)\n",
    "            sta_from_ap_1_sum_rssi = float(edge_ap_sta_11['rssi_sum'])\n",
    "            useful_rssi_list = []\n",
    "            interference_rssi_list = []\n",
    "            scale_factors = []\n",
    "            useful_rssi_list.append(sta_from_ap_1_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_0_sum_rssi)\n",
    "            edge_ap_ap_01 = find_edge_by_src_dst(edge_ap_ap,0,1)\n",
    "            edge_ap_ap_10 = find_edge_by_src_dst(edge_ap_ap,1,0)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_01['rssi_sum'])+float(edge_ap_ap_10['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[0]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[0]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[0]['ed']-ap_ap_rssi_max)/(sta_dicts_list[0]['ed']-sta_dicts_list[0]['pd']))\n",
    "            sta_dicts_list[0]['sinr'] = calculate_sinr(useful_rssi_list,interference_rssi_list,scale_factors)\n",
    "            \n",
    "            edge = find_edge_by_src_dst(edge_ap_sta,0,0)            \n",
    "            sta_from_ap_0_sum_rssi = float(edge['rssi_sum'])\n",
    "            edge = find_edge_by_src_dst(edge_ap_sta,1,0)\n",
    "            sta_from_ap_1_sum_rssi = float(edge['rssi_sum'])\n",
    "            useful_rssi_list = []\n",
    "            interference_rssi_list = []\n",
    "            useful_rssi_list.append(sta_from_ap_0_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_1_sum_rssi)\n",
    "            edge_ap_ap_01 = find_edge_by_src_dst(edge_ap_ap,0,1)\n",
    "            edge_ap_ap_10 = find_edge_by_src_dst(edge_ap_ap,1,0)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_01['rssi_sum'])+float(edge_ap_ap_10['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[0]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[0]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[0]['ed']-ap_ap_rssi_max)/(sta_dicts_list[0]['ed']-sta_dicts_list[0]['pd']))\n",
    "            \n",
    "            sta_dicts_list[1]['sinr'] = calculate_sinr(useful_rssi_list,interference_rssi_list,scale_factors)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=sta_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(sta_dicts_list)\n",
    "        sta_dicts_list.clear()\n",
    "        sta_dict1.clear()\n",
    "        sta_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三节点 sta 节点属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_name = 'test_set_2_3ap'\n",
    "\n",
    "data = []\n",
    "with codecs.open('./Bdata/train/'+src_file_name+'.csv',encoding='utf-8-sig') as f:\n",
    "    for row in csv.DictReader(f,skipinitialspace=True):\n",
    "        data.append(row)\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%3==0) :\n",
    "        sta_dicts_list = []\n",
    "        sta_dict1 = {}\n",
    "        \n",
    "        row = data[i]\n",
    "        sta_dict1['type'] = 1\n",
    "        # ap_dict1['id'] = row['ap_id']\n",
    "        # ap_dict1['loc_id'] = row['loc_id']\n",
    "        sta_dict1['id'] = int(row['sta_id'][4:len(row['sta_id'])])\n",
    "        sta_dict1['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        sta_dict1['protocol'] = 0\n",
    "        sta_dict1['bss_id'] = row['bss_id']\n",
    "        sta_dict1['pd'] = int(row['pd'])\n",
    "        sta_dict1['ed'] = int(row['ed'])\n",
    "        sta_dict1['nav'] = int(row['nav'])\n",
    "        sta_dict1['eirp'] = 0\n",
    "        sta_dict1['sinr'] = 0\n",
    "        \n",
    "        sta_dicts_list.append(sta_dict1)\n",
    "\n",
    "        sta_dict = {}\n",
    "        row = data[i+1]\n",
    "        sta_dict['type'] = 1\n",
    "        # ap_dict1['id'] = row['ap_id']\n",
    "        # ap_dict1['loc_id'] = row['loc_id']\n",
    "        sta_dict['id'] = int(row['sta_id'][4:len(row['sta_id'])])\n",
    "        sta_dict['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        sta_dict['protocol'] = 0\n",
    "        sta_dict['bss_id'] = row['bss_id']\n",
    "        sta_dict['pd'] = int(row['pd'])\n",
    "        sta_dict['ed'] = int(row['ed'])\n",
    "        sta_dict['nav'] = int(row['nav']) \n",
    "        sta_dict['eirp'] = 0\n",
    "        sta_dict['sinr'] = 0\n",
    "             \n",
    "        sta_dicts_list.append(sta_dict)\n",
    "\n",
    "        sta_dict2 = {}\n",
    "        row = data[i+2]\n",
    "        sta_dict2['type'] = 1\n",
    "        # ap_dict1['id'] = row['ap_id']\n",
    "        # ap_dict1['loc_id'] = row['loc_id']\n",
    "        sta_dict2['id'] = int(row['sta_id'][4:len(row['sta_id'])])\n",
    "        sta_dict2['loc_id'] = int(row['loc_id'][3:len(row['loc_id'])])\n",
    "        sta_dict2['protocol'] = 0\n",
    "        sta_dict2['bss_id'] = row['bss_id']\n",
    "        sta_dict2['pd'] = int(row['pd'])\n",
    "        sta_dict2['ed'] = int(row['ed'])\n",
    "        sta_dict2['nav'] = int(row['nav']) \n",
    "        sta_dict2['eirp'] = 0\n",
    "        sta_dict2['sinr'] = 0\n",
    "             \n",
    "        sta_dicts_list.append(sta_dict2)\n",
    "        \n",
    "        for j in range(120):\n",
    "            folder_path = \"./out/\"+src_file_name+\"/node_sta/test\"+str(i//3)\n",
    "            filename = folder_path + \"/nodes_sta_\" + str(j).zfill(3) + \".csv\"\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            edge_ap_sta = []\n",
    "            with codecs.open('./out/'+src_file_name+'/edge_ap_sta/test'+str(i//3)+'/ap_sta_'+str(j).zfill(3)+'.csv',encoding='utf-8-sig') as f:\n",
    "                for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                    edge_ap_sta.append(row)\n",
    "            edge_ap_ap = []\n",
    "            with codecs.open('./out/'+src_file_name+'/edge_ap_ap/test'+str(i//3)+'/ap_ap_'+str(j).zfill(3)+'.csv',encoding='utf-8-sig') as f:\n",
    "                for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                    edge_ap_ap.append(row)\n",
    "            \n",
    "            edge_ap_sta_02 = find_edge_by_src_dst(edge_ap_sta,0,2)            \n",
    "            sta_from_ap_0_sum_rssi = float(edge_ap_sta_02['rssi_sum'])\n",
    "            edge_ap_sta_12 = find_edge_by_src_dst(edge_ap_sta,1,2)\n",
    "            sta_from_ap_1_sum_rssi = float(edge_ap_sta_12['rssi_sum'])\n",
    "            edge_ap_sta_22 = find_edge_by_src_dst(edge_ap_sta,2,2)\n",
    "            sta_from_ap_2_sum_rssi = float(edge_ap_sta_22['rssi_sum'])\n",
    "            useful_rssi_list = []\n",
    "            interference_rssi_list = []\n",
    "            scale_factors = []\n",
    "            useful_rssi_list.append(sta_from_ap_2_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_0_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_1_sum_rssi)\n",
    "            edge_ap_ap_02 = find_edge_by_src_dst(edge_ap_ap,0,2)\n",
    "            edge_ap_ap_20 = find_edge_by_src_dst(edge_ap_ap,2,0)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_02['rssi_sum'])+float(edge_ap_ap_20['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[0]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[0]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[0]['ed']-ap_ap_rssi_max)/(sta_dicts_list[0]['ed']-sta_dicts_list[0]['pd']))\n",
    "            edge_ap_ap_12 = find_edge_by_src_dst(edge_ap_ap,1,2)\n",
    "            edge_ap_ap_21 = find_edge_by_src_dst(edge_ap_ap,2,1)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_21['rssi_sum'])+float(edge_ap_ap_12['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[0]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[0]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[0]['ed']-ap_ap_rssi_max)/(sta_dicts_list[0]['ed']-sta_dicts_list[0]['pd']))\n",
    "            sta_dicts_list[0]['sinr'] = calculate_sinr(useful_rssi_list,interference_rssi_list,scale_factors)\n",
    "            \n",
    "            edge_ap_sta_01 = find_edge_by_src_dst(edge_ap_sta,0,1)            \n",
    "            sta_from_ap_0_sum_rssi = float(edge_ap_sta_01['rssi_sum'])\n",
    "            edge_ap_sta_11 = find_edge_by_src_dst(edge_ap_sta,1,1)\n",
    "            sta_from_ap_1_sum_rssi = float(edge_ap_sta_11['rssi_sum'])\n",
    "            edge_ap_sta_21 = find_edge_by_src_dst(edge_ap_sta,2,1)\n",
    "            sta_from_ap_2_sum_rssi = float(edge_ap_sta_21['rssi_sum'])\n",
    "            useful_rssi_list = []\n",
    "            interference_rssi_list = []\n",
    "            scale_factors = []\n",
    "            useful_rssi_list.append(sta_from_ap_1_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_0_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_2_sum_rssi)\n",
    "            edge_ap_ap_01 = find_edge_by_src_dst(edge_ap_ap,0,1)\n",
    "            edge_ap_ap_10 = find_edge_by_src_dst(edge_ap_ap,1,0)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_01['rssi_sum'])+float(edge_ap_ap_10['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[1]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[1]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[1]['ed']-ap_ap_rssi_max)/(sta_dicts_list[1]['ed']-sta_dicts_list[1]['pd']))\n",
    "            edge_ap_ap_21 = find_edge_by_src_dst(edge_ap_ap,2,1)\n",
    "            edge_ap_ap_12 = find_edge_by_src_dst(edge_ap_ap,1,2)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_21['rssi_sum'])+float(edge_ap_ap_12['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[1]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[1]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[1]['ed']-ap_ap_rssi_max)/(sta_dicts_list[1]['ed']-sta_dicts_list[1]['pd']))\n",
    "                \n",
    "            sta_dicts_list[1]['sinr'] = calculate_sinr(useful_rssi_list,interference_rssi_list,scale_factors)\n",
    "            \n",
    "\n",
    "            \n",
    "            edge = find_edge_by_src_dst(edge_ap_sta,0,0)            \n",
    "            sta_from_ap_0_sum_rssi = float(edge['rssi_sum'])\n",
    "            edge = find_edge_by_src_dst(edge_ap_sta,1,0)\n",
    "            sta_from_ap_1_sum_rssi = float(edge['rssi_sum'])\n",
    "            edge = find_edge_by_src_dst(edge_ap_sta,2,0)\n",
    "            sta_from_ap_2_sum_rssi = float(edge['rssi_sum'])\n",
    "            useful_rssi_list = []\n",
    "            interference_rssi_list = []\n",
    "            useful_rssi_list.append(sta_from_ap_0_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_1_sum_rssi)\n",
    "            interference_rssi_list.append(sta_from_ap_2_sum_rssi)\n",
    "            edge_ap_ap_10 = find_edge_by_src_dst(edge_ap_ap,1,0)\n",
    "            edge_ap_ap_01 = find_edge_by_src_dst(edge_ap_ap,0,1)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_10['rssi_sum'])+float(edge_ap_ap_01['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[2]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[2]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[2]['ed']-ap_ap_rssi_max)/(sta_dicts_list[2]['ed']-sta_dicts_list[2]['pd']))\n",
    "            edge_ap_ap_20 = find_edge_by_src_dst(edge_ap_ap,2,0)\n",
    "            edge_ap_ap_02 = find_edge_by_src_dst(edge_ap_ap,0,2)\n",
    "            ap_ap_rssi_max = 0.5*(float(edge_ap_ap_20['rssi_sum'])+float(edge_ap_ap_02['rssi_sum']))\n",
    "            if (ap_ap_rssi_max > sta_dicts_list[2]['ed']) :\n",
    "                scale_factors.append(0)\n",
    "            elif (ap_ap_rssi_max < sta_dicts_list[2]['pd']) :\n",
    "                scale_factors.append(1)\n",
    "            else :\n",
    "                scale_factors.append(1-(sta_dicts_list[2]['ed']-ap_ap_rssi_max)/(sta_dicts_list[2]['ed']-sta_dicts_list[2]['pd']))\n",
    "            \n",
    "            sta_dicts_list[2]['sinr'] = calculate_sinr(useful_rssi_list,interference_rssi_list,scale_factors)\n",
    "            \n",
    "            with open(filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=sta_dicts_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(sta_dicts_list)\n",
    "        sta_dicts_list.clear()\n",
    "        sta_dict1.clear()\n",
    "        sta_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGL 图构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练图数据 v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'training_set_2ap_loc0_nav86'\n",
    "top_folder = './out/'+table_name+'/'\n",
    "sub_folder = 'edge_ap_ap/'\n",
    "\n",
    "ap_num = 2\n",
    "\n",
    "edge_ap_ap_path=[]\n",
    "edge_ap_sta_path=[]\n",
    "edge_sta_ap_path=[]\n",
    "\n",
    "for i in range(120):\n",
    "    file_name = 'ap_ap_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'edge_ap_sta/'\n",
    "for i in range(120):\n",
    "    file_name = 'ap_sta_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_sta_path.append(files)\n",
    "\n",
    "sub_folder = 'edge_sta_ap/'\n",
    "for i in range(120):\n",
    "    file_name = 'sta_ap_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_sta_ap_path.append(files)\n",
    "\n",
    "node_ap_path=[]\n",
    "node_sta_path=[]\n",
    "\n",
    "sub_folder = 'node_ap/'\n",
    "for i in range(120):\n",
    "    file_name = 'nodes_ap_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'node_sta/'\n",
    "for i in range(120):\n",
    "    file_name = 'nodes_sta_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_sta_path.append(files)\n",
    "\n",
    "ap_predict_path = []\n",
    "sub_folder = 'node_ap_predict/'\n",
    "for i in range(120):\n",
    "    file_name = 'nodes_ap_predict_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    ap_predict_path.append(files)\n",
    "\n",
    "\n",
    "# 每个数据表得到120个graph\n",
    "for i in range(120):\n",
    "    # 得到 ap-ap 边\n",
    "    ap_ap_src = []\n",
    "    ap_ap_dst = []\n",
    "    ap_ap_feat = []\n",
    "    # 得到 ap-sta 边\n",
    "    ap_sta_src = []\n",
    "    ap_sta_dst = []\n",
    "    ap_sta_feat = []\n",
    "    # 得到 sta-ap 边\n",
    "    sta_ap_src = []\n",
    "    sta_ap_dst = []\n",
    "    sta_ap_feat = []\n",
    "    # 得到ap节点的特征\n",
    "    ap_feat = []\n",
    "    # 得到sta节点的特征\n",
    "    sta_feat = []\n",
    "    # ap的预测结果\n",
    "    ap_predict = []\n",
    "    # 每个 graph 包含 test_count 张图\n",
    "    for j in range(len(edge_ap_ap_path[i])): \n",
    "        with codecs.open(edge_ap_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_ap_src.append(row['src'])\n",
    "                ap_ap_dst.append(row['dst'])\n",
    "                ap_ap_feat.append(row)\n",
    "        with codecs.open(edge_ap_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_sta_src.append(row['src'])\n",
    "                ap_sta_dst.append(row['dst'])\n",
    "                ap_sta_feat.append(row)\n",
    "        with codecs.open(edge_sta_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                sta_ap_src.append(row['src'])\n",
    "                sta_ap_dst.append(row['dst'])\n",
    "                sta_ap_feat.append(row)\n",
    "        with codecs.open(node_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_feat.append(row)\n",
    "        with codecs.open(node_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                sta_feat.append(row)\n",
    "        with codecs.open(ap_predict_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_predict.append(row)\n",
    "    \n",
    "    # 异构图的边集\n",
    "    edge_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): (torch.tensor(ap_ap_src), torch.tensor(ap_ap_dst)),\n",
    "        ('ap', 'ap-sta', 'sta'): (torch.tensor(ap_sta_src), torch.tensor(ap_sta_dst)),\n",
    "        ('sta', 'sta-ap', 'ap'): (torch.tensor(sta_ap_src), torch.tensor(sta_ap_dst))\n",
    "    }\n",
    "    g = dgl.heterograph(edge_dict)\n",
    "    # 异构图的节点预测集\n",
    "    ap_predict.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 异构图的节点特征集\n",
    "    ap_feat.sort(key=lambda x: x['id'])\n",
    "    sta_feat.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 把预测集中的 nss mcs 放进特征集\n",
    "    predict_dict = {item['id']:item for item in ap_predict}\n",
    "    for feat in ap_feat:\n",
    "        ap_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[ap_id]['mcs']\n",
    "        feat['nss'] = predict_dict[ap_id]['nss']\n",
    "    for feat in sta_feat:\n",
    "        sta_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[sta_id]['mcs']\n",
    "        feat['nss'] = predict_dict[sta_id]['nss']\n",
    "        \n",
    "    ap_feat = [list(row.values()) for row in ap_feat]\n",
    "    sta_feat = [list(row.values()) for row in sta_feat]\n",
    "    \n",
    "    node_feature_dict = {\n",
    "        'ap': torch.tensor(ap_feat),\n",
    "        'sta': torch.tensor(sta_feat)\n",
    "    }\n",
    "\n",
    "    ap_predict = [list(row.values()) for row in ap_predict]\n",
    "    node_predict_dict = {\n",
    "        'ap': torch.tensor(ap_predict),\n",
    "        'sta': torch.tensor(ap_predict)\n",
    "    }\n",
    "    \n",
    "    # 异构图的边特征集\n",
    "    ap_ap_feat = [list(row.values()) for row in ap_ap_feat]\n",
    "    ap_sta_feat = [list(row.values()) for row in ap_sta_feat]\n",
    "    sta_ap_feat = [list(row.values()) for row in sta_ap_feat]\n",
    "    edge_feature_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): torch.tensor(ap_ap_feat),\n",
    "        ('ap', 'ap-sta', 'sta'): torch.tensor(ap_sta_feat),\n",
    "        ('sta', 'sta-ap', 'ap'): torch.tensor(sta_ap_feat)\n",
    "    }\n",
    "    # 构造DGL异构图\n",
    "    mask_dict = {\n",
    "        'ap' : torch.ones(len(ap_feat),1),\n",
    "        'sta' : torch.zeros(len(sta_feat),1)\n",
    "    }\n",
    "    g.ndata['feat'] = node_feature_dict\n",
    "    g.ndata['predict'] = node_predict_dict\n",
    "    g.ndata['mask'] = mask_dict\n",
    "    g.edata['feat'] = edge_feature_dict\n",
    "\n",
    "    folder_pth = './HTNetDataset/'+table_name\n",
    "    file_name = folder_pth+'/train_'+str(i)+'.bin'\n",
    "    os.makedirs(folder_pth, exist_ok=True)\n",
    "    dgl.save_graphs(file_name, g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试图数据 v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'test_set_2_3ap'\n",
    "top_folder = './out/'+table_name+'/'\n",
    "sub_folder = 'edge_ap_ap/'\n",
    "\n",
    "ap_num = 3\n",
    "\n",
    "edge_ap_ap_path=[]\n",
    "edge_ap_sta_path=[]\n",
    "edge_sta_ap_path=[]\n",
    "\n",
    "for i in range(120):\n",
    "    file_name = 'ap_ap_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'edge_ap_sta/'\n",
    "for i in range(120):\n",
    "    file_name = 'ap_sta_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_sta_path.append(files)\n",
    "\n",
    "sub_folder = 'edge_sta_ap/'\n",
    "for i in range(120):\n",
    "    file_name = 'sta_ap_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_sta_ap_path.append(files)\n",
    "\n",
    "node_ap_path=[]\n",
    "node_sta_path=[]\n",
    "\n",
    "sub_folder = 'node_ap/'\n",
    "for i in range(120):\n",
    "    file_name = 'nodes_ap_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'node_sta/'\n",
    "for i in range(120):\n",
    "    file_name = 'nodes_sta_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_sta_path.append(files)\n",
    "\n",
    "ap_predict_path = []\n",
    "sub_folder = 'node_ap_predict/'\n",
    "for i in range(120):\n",
    "    file_name = 'nodes_ap_predict_'+str(i).zfill(3)+'.csv'\n",
    "    files = []\n",
    "    length = len(os.listdir(top_folder+sub_folder))\n",
    "    for j in range(length):\n",
    "        folder_pth = \"test\"+str(j)\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    ap_predict_path.append(files)\n",
    "\n",
    "\n",
    "# 每个数据表得到120个graph\n",
    "for i in range(120):\n",
    "    # 得到 ap-ap 边\n",
    "    ap_ap_src = []\n",
    "    ap_ap_dst = []\n",
    "    ap_ap_feat = []\n",
    "    # 得到 ap-sta 边\n",
    "    ap_sta_src = []\n",
    "    ap_sta_dst = []\n",
    "    ap_sta_feat = []\n",
    "    # 得到 sta-ap 边\n",
    "    sta_ap_src = []\n",
    "    sta_ap_dst = []\n",
    "    sta_ap_feat = []\n",
    "    # 得到ap节点的特征\n",
    "    ap_feat = []\n",
    "    # 得到sta节点的特征\n",
    "    sta_feat = []\n",
    "    # ap的预测结果\n",
    "    ap_predict = []\n",
    "    # 每个 graph 包含 test_count 张图\n",
    "    for j in range(len(edge_ap_ap_path[i])): \n",
    "        with codecs.open(edge_ap_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_ap_src.append(row['src'])\n",
    "                ap_ap_dst.append(row['dst'])\n",
    "                ap_ap_feat.append(row)\n",
    "        with codecs.open(edge_ap_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_sta_src.append(row['src'])\n",
    "                ap_sta_dst.append(row['dst'])\n",
    "                ap_sta_feat.append(row)\n",
    "        with codecs.open(edge_sta_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                sta_ap_src.append(row['src'])\n",
    "                sta_ap_dst.append(row['dst'])\n",
    "                sta_ap_feat.append(row)\n",
    "        with codecs.open(node_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_feat.append(row)\n",
    "        with codecs.open(node_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                sta_feat.append(row)\n",
    "        with codecs.open(ap_predict_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_predict.append(row)\n",
    "    \n",
    "    # 异构图的边集\n",
    "    edge_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): (torch.tensor(ap_ap_src), torch.tensor(ap_ap_dst)),\n",
    "        ('ap', 'ap-sta', 'sta'): (torch.tensor(ap_sta_src), torch.tensor(ap_sta_dst)),\n",
    "        ('sta', 'sta-ap', 'ap'): (torch.tensor(sta_ap_src), torch.tensor(sta_ap_dst))\n",
    "    }\n",
    "    g = dgl.heterograph(edge_dict)\n",
    "    # 异构图的节点预测集\n",
    "    ap_predict.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 异构图的节点特征集\n",
    "    ap_feat.sort(key=lambda x: x['id'])\n",
    "    sta_feat.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 把预测集中的 nss mcs 放进特征集\n",
    "    predict_dict = {item['id']:item for item in ap_predict}\n",
    "    for feat in ap_feat:\n",
    "        ap_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[ap_id]['mcs']\n",
    "        feat['nss'] = predict_dict[ap_id]['nss']\n",
    "    for feat in sta_feat:\n",
    "        sta_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[sta_id]['mcs']\n",
    "        feat['nss'] = predict_dict[sta_id]['nss']\n",
    "        \n",
    "    ap_feat = [list(row.values()) for row in ap_feat]\n",
    "    sta_feat = [list(row.values()) for row in sta_feat]\n",
    "    \n",
    "    node_feature_dict = {\n",
    "        'ap': torch.tensor(ap_feat),\n",
    "        'sta': torch.tensor(sta_feat)\n",
    "    }\n",
    "\n",
    "    node_predict_dict = {\n",
    "        'ap': torch.zeros(len(ap_feat),9),\n",
    "        'sta': torch.zeros(len(ap_feat),9)\n",
    "    }\n",
    "    \n",
    "    # 异构图的边特征集\n",
    "    ap_ap_feat = [list(row.values()) for row in ap_ap_feat]\n",
    "    ap_sta_feat = [list(row.values()) for row in ap_sta_feat]\n",
    "    sta_ap_feat = [list(row.values()) for row in sta_ap_feat]\n",
    "    edge_feature_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): torch.tensor(ap_ap_feat),\n",
    "        ('ap', 'ap-sta', 'sta'): torch.tensor(ap_sta_feat),\n",
    "        ('sta', 'sta-ap', 'ap'): torch.tensor(sta_ap_feat)\n",
    "    }\n",
    "    # 构造DGL异构图\n",
    "    mask_dict = {\n",
    "        'ap' : torch.ones(len(ap_feat),1),\n",
    "        'sta' : torch.zeros(len(sta_feat),1)\n",
    "    }\n",
    "    g.ndata['feat'] = node_feature_dict\n",
    "    g.ndata['predict'] = node_predict_dict\n",
    "    g.ndata['mask'] = mask_dict\n",
    "    g.edata['feat'] = edge_feature_dict\n",
    "\n",
    "    folder_pth = './HTNetDataset/'+table_name\n",
    "    file_name = folder_pth+'/train_'+str(i)+'.bin'\n",
    "    os.makedirs(folder_pth, exist_ok=True)\n",
    "    dgl.save_graphs(file_name, g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练图数据 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'training_set_2ap_loc0_nav86'\n",
    "top_folder = './out/'+table_name+'/'\n",
    "sub_folder = 'edge_ap_ap/'\n",
    "\n",
    "ap_num = 2\n",
    "\n",
    "edge_ap_ap_path=[]\n",
    "edge_ap_sta_path=[]\n",
    "edge_sta_ap_path=[]\n",
    "\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'ap_ap_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'edge_ap_sta/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'ap_sta_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_sta_path.append(files)\n",
    "\n",
    "\n",
    "sub_folder = 'edge_sta_ap/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'sta_ap_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_sta_ap_path.append(files)\n",
    "\n",
    "node_ap_path=[]\n",
    "node_sta_path=[]\n",
    "\n",
    "sub_folder = 'node_ap/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'nodes_ap_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'node_sta/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'nodes_sta_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_sta_path.append(files)\n",
    "\n",
    "ap_predict_path = []\n",
    "sub_folder = 'node_ap_predict/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'nodes_ap_predict_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    ap_predict_path.append(files)\n",
    "\n",
    "print(edge_ap_ap_path)\n",
    "# 每个数据表得到120个graph\n",
    "for i in range(len(edge_ap_ap_path)):\n",
    "    # 得到 ap-ap 边\n",
    "    ap_ap_src = []\n",
    "    ap_ap_dst = []\n",
    "    ap_ap_feat = []\n",
    "    # 得到 ap-sta 边\n",
    "    ap_sta_src = []\n",
    "    ap_sta_dst = []\n",
    "    ap_sta_feat = []\n",
    "    # 得到 sta-ap 边\n",
    "    sta_ap_src = []\n",
    "    sta_ap_dst = []\n",
    "    sta_ap_feat = []\n",
    "    # 得到ap节点的特征\n",
    "    ap_feat = []\n",
    "    # 得到sta节点的特征\n",
    "    sta_feat = []\n",
    "    # ap的预测结果\n",
    "    ap_predict = []\n",
    "    # 每个 graph 包含 test_count 张图\n",
    "    for j in range(len(edge_ap_ap_path[i])): \n",
    "        with codecs.open(edge_ap_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_ap_src.append(row['src'])\n",
    "                ap_ap_dst.append(row['dst'])\n",
    "                ap_ap_feat.append(row)\n",
    "        with codecs.open(edge_ap_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_sta_src.append(row['src'])\n",
    "                ap_sta_dst.append(row['dst'])\n",
    "                ap_sta_feat.append(row)\n",
    "        with codecs.open(edge_sta_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                sta_ap_src.append(row['src'])\n",
    "                sta_ap_dst.append(row['dst'])\n",
    "                sta_ap_feat.append(row)\n",
    "        with codecs.open(node_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_feat.append(row)\n",
    "        with codecs.open(node_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                sta_feat.append(row)\n",
    "        with codecs.open(ap_predict_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_predict.append(row)\n",
    "    \n",
    "    # 异构图的边集\n",
    "    edge_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): (torch.tensor(ap_ap_src), torch.tensor(ap_ap_dst)),\n",
    "        ('ap', 'ap-sta', 'sta'): (torch.tensor(ap_sta_src), torch.tensor(ap_sta_dst)),\n",
    "        ('sta', 'sta-ap', 'ap'): (torch.tensor(sta_ap_src), torch.tensor(sta_ap_dst))\n",
    "    }\n",
    "    g = dgl.heterograph(edge_dict)\n",
    "    # 异构图的节点预测集\n",
    "    ap_predict.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 异构图的节点特征集\n",
    "    ap_feat.sort(key=lambda x: x['id'])\n",
    "    sta_feat.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 把预测集中的 nss mcs 放进特征集\n",
    "    predict_dict = {item['id']:item for item in ap_predict}\n",
    "    for feat in ap_feat:\n",
    "        ap_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[ap_id]['mcs']\n",
    "        feat['nss'] = predict_dict[ap_id]['nss']\n",
    "    for feat in sta_feat:\n",
    "        sta_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[sta_id]['mcs']\n",
    "        feat['nss'] = predict_dict[sta_id]['nss']\n",
    "        \n",
    "    ap_feat = [list(row.values()) for row in ap_feat]\n",
    "    sta_feat = [list(row.values()) for row in sta_feat]\n",
    "    \n",
    "    node_feature_dict = {\n",
    "        'ap': torch.tensor(ap_feat),\n",
    "        'sta': torch.tensor(sta_feat)\n",
    "    }\n",
    "\n",
    "    ap_predict = [list(row.values()) for row in ap_predict]\n",
    "    node_predict_dict = {\n",
    "        'ap': torch.tensor(ap_predict),\n",
    "        'sta': torch.tensor(ap_predict)\n",
    "    }\n",
    "    \n",
    "    # 异构图的边特征集\n",
    "    ap_ap_feat = [list(row.values()) for row in ap_ap_feat]\n",
    "    ap_sta_feat = [list(row.values()) for row in ap_sta_feat]\n",
    "    sta_ap_feat = [list(row.values()) for row in sta_ap_feat]\n",
    "    edge_feature_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): torch.tensor(ap_ap_feat),\n",
    "        ('ap', 'ap-sta', 'sta'): torch.tensor(ap_sta_feat),\n",
    "        ('sta', 'sta-ap', 'ap'): torch.tensor(sta_ap_feat)\n",
    "    }\n",
    "    # 构造DGL异构图\n",
    "    mask_dict = {\n",
    "        'ap' : torch.ones(len(ap_feat),1),\n",
    "        'sta' : torch.zeros(len(sta_feat),1)\n",
    "    }\n",
    "    g.ndata['feat'] = node_feature_dict\n",
    "    g.ndata['predict'] = node_predict_dict\n",
    "    g.ndata['mask'] = mask_dict\n",
    "    g.edata['feat'] = edge_feature_dict\n",
    "\n",
    "    folder_pth = './HTNetDataset/'+table_name\n",
    "    file_name = folder_pth+'/train_'+str(i)+'.bin'\n",
    "    os.makedirs(folder_pth, exist_ok=True)\n",
    "    dgl.save_graphs(file_name, g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试图数据 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'test_set_2_2ap'\n",
    "top_folder = './out/'+table_name+'/'\n",
    "sub_folder = 'edge_ap_ap/'\n",
    "\n",
    "ap_num = 2\n",
    "\n",
    "edge_ap_ap_path=[]\n",
    "edge_ap_sta_path=[]\n",
    "edge_sta_ap_path=[]\n",
    "\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'ap_ap_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'edge_ap_sta/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'ap_sta_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_ap_sta_path.append(files)\n",
    "\n",
    "\n",
    "sub_folder = 'edge_sta_ap/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'sta_ap_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    edge_sta_ap_path.append(files)\n",
    "\n",
    "node_ap_path=[]\n",
    "node_sta_path=[]\n",
    "\n",
    "sub_folder = 'node_ap/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'nodes_ap_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_ap_path.append(files)\n",
    "\n",
    "sub_folder = 'node_sta/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'nodes_sta_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    node_sta_path.append(files)\n",
    "\n",
    "ap_predict_path = []\n",
    "sub_folder = 'node_ap_predict/'\n",
    "length = len(os.listdir(top_folder+sub_folder))\n",
    "for i in range(length):\n",
    "    files = []\n",
    "    folder_pth = \"test\"+str(i)\n",
    "    for j in range(120):\n",
    "        file_name = 'nodes_ap_predict_'+str(j).zfill(3)+'.csv'\n",
    "        files.append(top_folder+sub_folder+folder_pth+'/'+file_name)\n",
    "    ap_predict_path.append(files)\n",
    "\n",
    "print(ap_predict_path)\n",
    "# 每个数据表得到120个graph\n",
    "for i in range(len(edge_ap_ap_path)):\n",
    "    # 得到 ap-ap 边\n",
    "    ap_ap_src = []\n",
    "    ap_ap_dst = []\n",
    "    ap_ap_feat = []\n",
    "    # 得到 ap-sta 边\n",
    "    ap_sta_src = []\n",
    "    ap_sta_dst = []\n",
    "    ap_sta_feat = []\n",
    "    # 得到 sta-ap 边\n",
    "    sta_ap_src = []\n",
    "    sta_ap_dst = []\n",
    "    sta_ap_feat = []\n",
    "    # 得到ap节点的特征\n",
    "    ap_feat = []\n",
    "    # 得到sta节点的特征\n",
    "    sta_feat = []\n",
    "    # ap的预测结果\n",
    "    ap_predict = []\n",
    "    # 每个 graph 包含 test_count 张图\n",
    "    for j in range(len(edge_ap_ap_path[i])): \n",
    "        with codecs.open(edge_ap_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_ap_src.append(row['src'])\n",
    "                ap_ap_dst.append(row['dst'])\n",
    "                ap_ap_feat.append(row)\n",
    "        with codecs.open(edge_ap_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                ap_sta_src.append(row['src'])\n",
    "                ap_sta_dst.append(row['dst'])\n",
    "                ap_sta_feat.append(row)\n",
    "        with codecs.open(edge_sta_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['src'] = int(row['src'])+j*ap_num\n",
    "                row['dst'] = int(row['dst'])+j*ap_num\n",
    "                sta_ap_src.append(row['src'])\n",
    "                sta_ap_dst.append(row['dst'])\n",
    "                sta_ap_feat.append(row)\n",
    "        with codecs.open(node_ap_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_feat.append(row)\n",
    "        with codecs.open(node_sta_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                sta_feat.append(row)\n",
    "        with codecs.open(ap_predict_path[i][j], 'r', encoding='utf-8-sig') as f:\n",
    "            for row in csv.DictReader(f,skipinitialspace=True):\n",
    "                print(row)\n",
    "                row = {key: np.float32(value) for key, value in row.items()}\n",
    "                row['id'] = row['id']+j*ap_num\n",
    "                ap_predict.append(row)\n",
    "    \n",
    "    # 异构图的边集\n",
    "    edge_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): (torch.tensor(ap_ap_src), torch.tensor(ap_ap_dst)),\n",
    "        ('ap', 'ap-sta', 'sta'): (torch.tensor(ap_sta_src), torch.tensor(ap_sta_dst)),\n",
    "        ('sta', 'sta-ap', 'ap'): (torch.tensor(sta_ap_src), torch.tensor(sta_ap_dst))\n",
    "    }\n",
    "    g = dgl.heterograph(edge_dict)\n",
    "    # 异构图的节点预测集\n",
    "    ap_predict.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 异构图的节点特征集\n",
    "    ap_feat.sort(key=lambda x: x['id'])\n",
    "    sta_feat.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # 把预测集中的 nss mcs 放进特征集\n",
    "    predict_dict = {item['id']:item for item in ap_predict}\n",
    "    for feat in ap_feat:\n",
    "        ap_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[ap_id]['mcs']\n",
    "        feat['nss'] = predict_dict[ap_id]['nss']\n",
    "    for feat in sta_feat:\n",
    "        sta_id = feat['id']\n",
    "        feat['mcs'] = predict_dict[sta_id]['mcs']\n",
    "        feat['nss'] = predict_dict[sta_id]['nss']\n",
    "        \n",
    "    ap_feat = [list(row.values()) for row in ap_feat]\n",
    "    sta_feat = [list(row.values()) for row in sta_feat]\n",
    "    \n",
    "    node_feature_dict = {\n",
    "        'ap': torch.tensor(ap_feat),\n",
    "        'sta': torch.tensor(sta_feat)\n",
    "    }\n",
    "\n",
    "    node_predict_dict = {\n",
    "        'ap': torch.zeros(len(ap_feat),9),\n",
    "        'sta': torch.zeros(len(ap_feat),9)\n",
    "    }\n",
    "    \n",
    "    # 异构图的边特征集\n",
    "    ap_ap_feat = [list(row.values()) for row in ap_ap_feat]\n",
    "    ap_sta_feat = [list(row.values()) for row in ap_sta_feat]\n",
    "    sta_ap_feat = [list(row.values()) for row in sta_ap_feat]\n",
    "    edge_feature_dict = {\n",
    "        ('ap', 'ap-ap', 'ap'): torch.tensor(ap_ap_feat),\n",
    "        ('ap', 'ap-sta', 'sta'): torch.tensor(ap_sta_feat),\n",
    "        ('sta', 'sta-ap', 'ap'): torch.tensor(sta_ap_feat)\n",
    "    }\n",
    "    # 构造DGL异构图\n",
    "    mask_dict = {\n",
    "        'ap' : torch.ones(len(ap_feat),1),\n",
    "        'sta' : torch.zeros(len(sta_feat),1)\n",
    "    }\n",
    "    g.ndata['feat'] = node_feature_dict\n",
    "    g.ndata['predict'] = node_predict_dict\n",
    "    g.ndata['mask'] = mask_dict\n",
    "    g.edata['feat'] = edge_feature_dict\n",
    "\n",
    "    folder_pth = './HTNetDataset/'+table_name\n",
    "    file_name = folder_pth+'/train_'+str(i)+'.bin'\n",
    "    os.makedirs(folder_pth, exist_ok=True)\n",
    "    dgl.save_graphs(file_name, g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HTNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
